################################################################################
# Front‑end Dockerfile (Yew / Trunk)                                            #
#                                                                               #
# The image is optimized for day‑to‑day development with docker‑compose:         #
#  • `cargo‑chef` is employed to compile the dependency graph only once.         #
#  • The resulting target directory is copied to the final stage so that        #
#    subsequent `trunk build / serve` invocations are almost instantaneous.      #
#                                                                               #
# The final stage keeps the Rust tool‑chain and Trunk around because we rely on  #
# live‑reload (`trunk serve`) during local development.                          #
################################################################################

# ──────────────────────────────────────────────────────────────────────────────
# Stage 1 – plan (collect dependency graph)                                    ─
# -----------------------------------------------------------------------------
FROM rust:1-alpine AS chef-planner

RUN apk add --no-cache musl-dev openssl-dev pkgconfig build-base git && \
    cargo install cargo-chef --locked && \
    cargo install trunk --locked

WORKDIR /app

# Copy minimal project manifest files.
# Copy workspace manifests – cargo‑chef only needs *Cargo.toml* files (and
# optionally *Cargo.lock*) to resolve the full dependency graph.
# The root workspace definition must be present, otherwise `cargo metadata`
# errors out ("could not find Cargo.toml in /app"), breaking the build.
COPY Cargo.toml ./
COPY Cargo.lock ./
COPY Trunk.toml ./
# Crate manifests referenced by the workspace
COPY backend/Cargo.toml backend/
COPY frontend/Cargo.toml frontend/

# Provide a minimal crate root so that `cargo metadata` succeeds without the
# full source tree (which is added later for caching efficiency).
RUN mkdir -p frontend/src && echo "pub fn _dummy() {}" > frontend/src/lib.rs

# Generate the deterministic recipe describing the full dependency graph.
RUN cargo chef prepare --recipe-path recipe.json


# ──────────────────────────────────────────────────────────────────────────────
# Stage 2 – cook (compile cached dependencies)                                 ─
# -----------------------------------------------------------------------------
FROM rust:1-alpine AS chef-cook
RUN apk add --no-cache musl-dev openssl-dev pkgconfig build-base git && \
    cargo install cargo-chef --locked && \
    cargo install trunk --locked && \
    rustup target add wasm32-unknown-unknown

WORKDIR /app

# Build the whole dependency graph for the WebAssembly target.
COPY --from=chef-planner /app/recipe.json ./recipe.json
# Build only the frontend crate for the WebAssembly target. Compiling the entire
# workspace would attempt to build the backend crate for `wasm32-unknown-unknown`
# which fails (e.g. `mio` does not support that target). Limiting the scope to
# the `stovoy-tech-frontend` package avoids those unsupported dependencies while
# still caching all artefacts required for incremental rebuilds.
RUN cargo chef cook --release --recipe-path recipe.json \
    --target wasm32-unknown-unknown \
    --package stovoy-tech-frontend


# ──────────────────────────────────────────────────────────────────────────────
# Stage 3 – runtime / development                                              ─
# -----------------------------------------------------------------------------
# We keep Rust and Trunk installed in the final image so that `trunk serve` can
# watch & rebuild on every file change when run via docker‑compose.
FROM rust:1-alpine AS runtime

RUN apk add --no-cache musl-dev openssl-dev pkgconfig build-base git && \
    cargo install trunk --locked && \
    rustup target add wasm32-unknown-unknown

WORKDIR /app

# Bring over the cached build artefacts so that only changed crates need to be
# recompiled when the container is started.
COPY --from=chef-cook /app/target /app/target

# Full project source (mounted as a volume by docker‑compose during dev).
COPY . .

EXPOSE 8081

# The command matches the original compose definition.
CMD ["trunk", "serve", "--watch", ".", "--config", "Trunk.toml", "--public-url", "/", "--address", "0.0.0.0"]


# ──────────────────────────────────────────────────────────────────────────────
# Stage 4 – production static site build                                       ─
# -----------------------------------------------------------------------------
# Build the optimized static assets that will be served by Caddy in production.
# We keep this stage separate so that local development (runtime stage) is not
# slowed down by a full release build.

FROM runtime AS site-build

# Produce the release‑optimized static site into /site
RUN trunk build --release --dist /site --public-url /


# ──────────────────────────────────────────────────────────────────────────────
# Stage 5 – final Caddy image (production)                                     ─
# -----------------------------------------------------------------------------
FROM caddy:2-alpine AS caddy

# Copy Caddyfile for production
COPY Caddyfile.prod /etc/caddy/Caddyfile

# Copy pre‑built static site generated in the previous stage
COPY --from=site-build /site /site

# Caddy listens on 80/443 by default; nothing else to do – the container start
# command is provided by the base image.
